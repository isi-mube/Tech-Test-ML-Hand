{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0137223-1ca0-42aa-8270-f5958fb44e63",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #00BFFF;\">00 |</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8bee598-9b44-41eb-8a6c-ed9051701450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ðŸ“š Basic libraries\n",
    "import os # file access\n",
    "import matplotlib.pyplot as plt #  visualization\n",
    "import numpy as np # image numeric array manipulation\n",
    "import shutil # folder operations\n",
    "import random # shuffle data\n",
    "\n",
    "# âš™ï¸ Settings\n",
    "import warnings # who likes warnings?\n",
    "warnings.filterwarnings('ignore') # ignore warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # ignoring AVX warning from tensorflow\n",
    "\n",
    "# ðŸŒ Computer Vision\n",
    "import cv2 # image reading\n",
    "from tensorflow import keras # deep learning and neural networks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # data augmentation\n",
    "from tensorflow.keras.utils import img_to_array, array_to_img, load_img # saving augmented Data\n",
    "from tensorflow.keras import layers, Model # neural network and model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ccc02d-866e-4efb-bdc6-d23ddd0ec4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Specific Functions\n",
    "def augment_images(data_dir, datagen, num_images=5):\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".jpeg\"):\n",
    "            img_path = os.path.join(data_dir, filename)\n",
    "            img_array = img_to_array(load_img(img_path))  # converting the image to numpy array\n",
    "            img_array = img_array.reshape((1,) + img_array.shape)  # reshape the data\n",
    "\n",
    "            # Generate new images\n",
    "            for i, _ in enumerate(datagen.flow(img_array, batch_size=1, save_to_dir=data_dir, save_prefix='aug', save_format='jpeg')):\n",
    "                if i >= num_images - 1:\n",
    "                    break\n",
    "                    \n",
    "def unet_model(input_shape): # adapted from Keras\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Entry block\n",
    "    x = keras.layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "    \n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = keras.layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = keras.layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "        \n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = keras.layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = keras.layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = keras.layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "        \n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed1d0c-0486-4a70-b7cf-a4c59f04c6ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #00BFFF;\">01 | Data Extraction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b43c9-d81e-4e54-a247-26119dc43626",
   "metadata": {},
   "source": [
    "This public hand dataset contains different folders (numeric) with two subfolders: no_bg and original.\n",
    "\n",
    "The goal is to segment hands from unseen data with high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b62c71a-cabd-4074-baab-38a1dc14dab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.path.expanduser(\"~\"), \"Documents\", \"1_projects\", \"01_hands\", \"01_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc49b5-ef53-4b61-8f0d-30e204bbfa5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #00BFFF;\">02 | Data Cleaning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b4478-5e6f-455c-8ba1-e4c253536df4",
   "metadata": {},
   "source": [
    "For full Data Cleaning Process documentation, refer to 02_new_prototype python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b33ffe-28c0-4363-bca8-4ebb953bec6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# folders for clean images:\n",
    "original_clean_path = os.path.join(data_path, '00_original_clean')\n",
    "no_bg_clean_path = os.path.join(data_path, '00_no_bg_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ffced-614a-495f-b641-99c0ede70857",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Training, Validation and Test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f46490-c7ba-4ffa-b291-fbda240e0899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original and mask directories for test\n",
    "test_dir = os.path.join(data_path, \"03_test\")\n",
    "\n",
    "# original and mask directories for train and validation\n",
    "train_dir = os.path.join(data_path, \"01_train\")\n",
    "val_dir = os.path.join(data_path, \"02_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c7796-c940-4d8a-9ea0-9c7aea29086e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 style=\"color: #008080;\">Training, Validation and Test split</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee394359-483f-462c-b991-2ec822826b6b",
   "metadata": {},
   "source": [
    "For this model, we use (as in 02_new_prototype python script):\n",
    "* 65% Data for Training (it'll be augmented)\n",
    "* 25% Data for Validation\n",
    "* 10% Data for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ea7ec-0d05-4140-ba83-c813e5166c8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Debugging ImageDataGenerator</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2028f1d-40cb-4e16-a990-66cc98386561",
   "metadata": {},
   "source": [
    "ImageDataGenerator wasn't detecting images. They need to be in subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254bdf94-f729-4e6b-8bba-dd587c68ee0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_original_dir = os.path.join(train_dir, \"original\")\n",
    "train_mask_dir = os.path.join(train_dir, \"mask\")\n",
    "\n",
    "val_original_dir = os.path.join(val_dir, \"original\")\n",
    "val_mask_dir = os.path.join(val_dir, \"mask\")\n",
    "\n",
    "test_original_dir = os.path.join(test_dir, \"original\")\n",
    "test_mask_dir = os.path.join(test_dir, \"mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512d81b-094d-4596-a1cc-38e4308db840",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #00BFFF;\">03 | Data Pre-Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf38183-a1e8-492b-8f70-b46ecaa0a488",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Histogram Equalization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae452cd-f64f-4342-bc62-3a3233807621",
   "metadata": {},
   "source": [
    "CLAHE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbbd94f-424a-4116-9813-0bf40c6c0070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def enhance_contrast(img):\n",
    "    # if the image is grayscale or not. If not, convert it.\n",
    "    if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "\n",
    "    gray = np.uint8(gray)\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)) # applying CLAHE\n",
    "    clahe_img = clahe.apply(gray)\n",
    "    \n",
    "    return clahe_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f4a292-78fc-4214-90f3-abfc6d079b82",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Morphological Operations:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589f917-c5a5-4136-882f-11d1e5cdb7d7",
   "metadata": {},
   "source": [
    "Dilatation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5141adf1-2836-4726-b362-fe55becbd27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_dilation(img, kernel_size=5, iterations=1):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    dilation = cv2.dilate(img, kernel, iterations=iterations)\n",
    "    return dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f048d8-cfc2-4044-87a7-e59ebb85546f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">All pre-processing methods:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39c18e97-c85f-4531-b86a-f091499200f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_preprocessing(img):\n",
    "    clahe_img = enhance_contrast(img)\n",
    "    dilation_img = apply_dilation(clahe_img)\n",
    "    \n",
    "    dilation_img_3ch = np.stack([dilation_img]*3, axis=-1) # single channel to a 3 channel image\n",
    "    \n",
    "    return dilation_img_3ch.astype(np.float32) # converting it as a float for rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ad196-8def-4804-9cfa-0001508a8d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Pixel Normalization + Pre-Processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d08f4612-44ed-4b04-997b-2b9db7ee2d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_image_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=custom_preprocessing)\n",
    "train_mask_datagen = ImageDataGenerator(rescale=1./255) # masks without pre-processing techniques\n",
    "\n",
    "val_image_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=custom_preprocessing)\n",
    "val_mask_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_image_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=custom_preprocessing)\n",
    "test_mask_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83fde2-5370-4f03-8445-f086326afcbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h2 style=\"color: #00BFFF;\">Data Augmentation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d804975-8cb0-4ad5-b660-7cbcc50e9d01",
   "metadata": {},
   "source": [
    "For full Data Augmentation process, refer to 02_new_protype python script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542565b-e697-4774-a61c-777106e74dca",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #00BFFF;\">04 | Modeling</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9a1fc-7381-4cf2-8acb-e56a4cfd7e0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Fine-Tunning the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e80664-5c21-43df-8419-ef7ef82e17f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters that we can fine-tune later on\n",
    "img_height = 480\n",
    "img_width = 272\n",
    "image_size = (img_height, img_width)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8a727-a9f6-4b1f-a695-6787c0ee8364",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Train and Validation Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d641f3-1ae7-4cd3-949e-61bb21b9d4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 303 images belonging to 1 classes.\n",
      "Found 303 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_image_gen = train_image_datagen.flow_from_directory(\n",
    "    train_original_dir,\n",
    "    class_mode=None,\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_mask_gen = train_mask_datagen.flow_from_directory(\n",
    "    train_mask_dir,\n",
    "    class_mode=None,\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb6c469-2ba2-4f09-bbbb-f7fff02ff80e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = zip(train_image_gen, train_mask_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "668c622a-2d4c-43ba-8e30-834b450fc785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 images belonging to 1 classes.\n",
      "Found 19 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "val_image_gen = val_image_datagen.flow_from_directory(\n",
    "    val_original_dir,\n",
    "    class_mode=None,\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "val_mask_gen = val_mask_datagen.flow_from_directory(\n",
    "    val_mask_dir,\n",
    "    class_mode=None,\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc783dd-f85a-45d4-94c2-e3387490592e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_val = zip(val_image_gen, val_mask_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74cebb-e4ee-4596-9162-4e49d9d45d46",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Model Build</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdf1c3fe-c90c-4e4f-81e5-3b5d9b715618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = unet_model(input_shape=(img_height, img_width, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2c9a2-2f02-4b37-a788-428d0956be25",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Hyperparameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e06ddec9-c323-407f-a0e1-8fcb46edb6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adamax(learning_rate=0.01),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", keras.metrics.MeanIoU(num_classes=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95af739-7162-45fc-bb47-2b6e4dff9b30",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Epochs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79044138-5787-4758-a93b-64e724b897e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_steps = len(train_image_gen)\n",
    "x_val_steps = len(val_image_gen)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,  # my patience for this\n",
    "    verbose=1,\n",
    "    restore_best_weights=True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e54cff-f503-4ae8-92fb-aa15119bbb1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Check Points</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "971a4acf-f813-4b27-a05c-42f631970718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoints = os.path.join(data_path, \"04_epochs\", \"save_at_II_{epoch}.keras\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(checkpoints),\n",
    "    early_stopping\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf198644-c579-4f22-8adc-554ba3c1e24e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Deep Learning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdb9e9eb-f076-40c1-8b42-aa944fdbb4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "38/38 [==============================] - 634s 16s/step - loss: 2.5127 - accuracy: 0.6627 - mean_io_u: 0.3875 - val_loss: 0.6664 - val_accuracy: 0.7269 - val_mean_io_u: 0.3676\n",
      "Epoch 2/30\n",
      "38/38 [==============================] - 4674s 126s/step - loss: 0.5805 - accuracy: 0.7461 - mean_io_u: 0.3835 - val_loss: 0.5955 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 3/30\n",
      "38/38 [==============================] - 3769s 100s/step - loss: 0.5674 - accuracy: 0.7489 - mean_io_u: 0.3835 - val_loss: 0.5928 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 4/30\n",
      "38/38 [==============================] - 3091s 83s/step - loss: 0.5594 - accuracy: 0.7491 - mean_io_u: 0.3835 - val_loss: 0.5807 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 5/30\n",
      "38/38 [==============================] - 620s 16s/step - loss: 0.5492 - accuracy: 0.7496 - mean_io_u: 0.3835 - val_loss: 0.5762 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 6/30\n",
      "38/38 [==============================] - 582s 15s/step - loss: 0.5372 - accuracy: 0.7500 - mean_io_u: 0.3835 - val_loss: 0.5790 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 7/30\n",
      "38/38 [==============================] - 580s 15s/step - loss: 0.5329 - accuracy: 0.7488 - mean_io_u: 0.3835 - val_loss: 0.5779 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 8/30\n",
      "38/38 [==============================] - 498s 13s/step - loss: 0.5278 - accuracy: 0.7495 - mean_io_u: 0.3835 - val_loss: 0.5750 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 9/30\n",
      "38/38 [==============================] - 303s 8s/step - loss: 0.5264 - accuracy: 0.7495 - mean_io_u: 0.3835 - val_loss: 0.5614 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 10/30\n",
      "38/38 [==============================] - 462s 12s/step - loss: 0.5271 - accuracy: 0.7491 - mean_io_u: 0.3835 - val_loss: 0.5656 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 11/30\n",
      "38/38 [==============================] - 367s 10s/step - loss: 0.5213 - accuracy: 0.7496 - mean_io_u: 0.3835 - val_loss: 0.5473 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 12/30\n",
      "38/38 [==============================] - 393s 10s/step - loss: 0.5187 - accuracy: 0.7492 - mean_io_u: 0.3835 - val_loss: 0.5283 - val_accuracy: 0.7300 - val_mean_io_u: 0.3676\n",
      "Epoch 13/30\n",
      "38/38 [==============================] - 339s 9s/step - loss: 0.5170 - accuracy: 0.7489 - mean_io_u: 0.3835 - val_loss: 0.5248 - val_accuracy: 0.7300 - val_mean_io_u: 0.3676\n",
      "Epoch 14/30\n",
      "38/38 [==============================] - 294s 8s/step - loss: 0.5158 - accuracy: 0.7486 - mean_io_u: 0.3835 - val_loss: 0.5068 - val_accuracy: 0.7300 - val_mean_io_u: 0.3676\n",
      "Epoch 15/30\n",
      "38/38 [==============================] - 282s 7s/step - loss: 0.5133 - accuracy: 0.7520 - mean_io_u: 0.3835 - val_loss: 0.4719 - val_accuracy: 0.7372 - val_mean_io_u: 0.3676\n",
      "Epoch 16/30\n",
      "38/38 [==============================] - 283s 7s/step - loss: 0.5103 - accuracy: 0.7509 - mean_io_u: 0.3835 - val_loss: 0.4725 - val_accuracy: 0.7356 - val_mean_io_u: 0.3676\n",
      "Epoch 17/30\n",
      "38/38 [==============================] - 283s 7s/step - loss: 0.5080 - accuracy: 0.7526 - mean_io_u: 0.3835 - val_loss: 0.4560 - val_accuracy: 0.7388 - val_mean_io_u: 0.3676\n",
      "Epoch 18/30\n",
      "38/38 [==============================] - 288s 8s/step - loss: 0.5079 - accuracy: 0.7535 - mean_io_u: 0.3835 - val_loss: 0.4628 - val_accuracy: 0.7412 - val_mean_io_u: 0.3676\n",
      "Epoch 19/30\n",
      "38/38 [==============================] - 281s 7s/step - loss: 0.5066 - accuracy: 0.7528 - mean_io_u: 0.3835 - val_loss: 0.4465 - val_accuracy: 0.7521 - val_mean_io_u: 0.3676\n",
      "Epoch 20/30\n",
      "38/38 [==============================] - 276s 7s/step - loss: 0.5047 - accuracy: 0.7539 - mean_io_u: 0.3835 - val_loss: 0.4498 - val_accuracy: 0.7477 - val_mean_io_u: 0.3676\n",
      "Epoch 21/30\n",
      "38/38 [==============================] - 281s 7s/step - loss: 0.5029 - accuracy: 0.7563 - mean_io_u: 0.3835 - val_loss: 0.5416 - val_accuracy: 0.7298 - val_mean_io_u: 0.3676\n",
      "Epoch 22/30\n",
      "38/38 [==============================] - 279s 7s/step - loss: 0.5013 - accuracy: 0.7557 - mean_io_u: 0.3835 - val_loss: 0.4782 - val_accuracy: 0.7356 - val_mean_io_u: 0.3676\n",
      "Epoch 23/30\n",
      "38/38 [==============================] - 278s 7s/step - loss: 0.5003 - accuracy: 0.7566 - mean_io_u: 0.3835 - val_loss: 0.4482 - val_accuracy: 0.7610 - val_mean_io_u: 0.3676\n",
      "Epoch 24/30\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.7536 - mean_io_u: 0.3835Restoring model weights from the end of the best epoch: 19.\n",
      "38/38 [==============================] - 276s 7s/step - loss: 0.5043 - accuracy: 0.7536 - mean_io_u: 0.3835 - val_loss: 0.5123 - val_accuracy: 0.7320 - val_mean_io_u: 0.3676\n",
      "Epoch 24: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    epochs=epochs, \n",
    "    steps_per_epoch=X_train_steps,\n",
    "    validation_data=x_val,\n",
    "    validation_steps=x_val_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90636268-b907-455b-aa97-6c4a0f849a3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #00BFFF;\">05 | Evaluating the Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d44e28a-084d-4a5d-b0a3-634469c27509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  path to best epoch\n",
    "checkpoint_path = os.path.join(data_path, \"04_epochs\", \"save_at_II_19.keras\")\n",
    "\n",
    "# load the model\n",
    "loaded_model = load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f27b1a8-03a3-4b26-9e6d-2ecd482995f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images belonging to 1 classes.\n",
      "Found 9 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image_gen = test_image_datagen.flow_from_directory(\n",
    "    test_original_dir,\n",
    "    class_mode=None,\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_mask_gen = test_mask_datagen.flow_from_directory(\n",
    "    test_mask_dir,\n",
    "    class_mode=None,\n",
    "    seed=1337,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "696f3c0d-533c-459f-9732-3f33e1d36f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = zip(test_image_gen, test_mask_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0dd06-0ddc-4deb-a32f-d7d419438db2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: #008080;\">Metrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e1fd68a-d3a2-4aec-8e39-7756f68dbd7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 328ms/step - loss: 0.4350 - accuracy: 0.7699 - mean_io_u: 0.3800\n",
      "Test Loss: 0.4350\n",
      "Test Accuracy: 0.7699\n",
      "Test MeanIoU: 0.3800\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "loss, accuracy, iou = loaded_model.evaluate(Y_test, steps=len(test_image_gen))\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test MeanIoU: {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94e6dafd-5153-4705-93d8-b6ade0c12b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, masks = next(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "588e3977-f2f5-4c5f-8770-4de0276dceb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# predict masks using the model\n",
    "predictions = loaded_model.predict(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fcdd0-f2f0-48e8-ba8e-428400937b51",
   "metadata": {
    "tags": []
   },
   "source": [
    "# plotting original, true masks, predicted masks, and segmented images\n",
    "for i in range(min(5, batch_size)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Display original image\n",
    "    plt.subplot(1, 4, 1)\n",
    "    image_to_display = (images[i] * 255).astype(np.uint8)\n",
    "    plt.imshow(image_to_display)\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    # Display true mask\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(masks[i].squeeze(), cmap='gray')\n",
    "    plt.title('True Mask')\n",
    "\n",
    "    # Display predicted mask\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
    "    plt.title('Predicted Mask')\n",
    "    \n",
    "    # Display segmented image\n",
    "    plt.subplot(1, 4, 4)\n",
    "    segmented_image = (images[i] * predictions[i].squeeze()[:, :, np.newaxis]).astype(np.uint8)\n",
    "    plt.imshow(segmented_image)\n",
    "    plt.title('Segmented Image')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afb6607-0034-4ca7-8b9e-1cc94a95038f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
